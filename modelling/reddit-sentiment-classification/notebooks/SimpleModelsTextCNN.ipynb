{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea0ad626-8721-4e44-a91e-1ac66da0c62a",
   "metadata": {},
   "source": [
    "# Training TextCNN classifier for sentimental analysis of Reddit data\n",
    "Training a deep learning model (TextCNN) that is based on 1D convolutional layers. Pre-trained [embeddings by glove](https://nlp.stanford.edu/projects/glove/) are used when setting up the model. TextCNN proposed in [this paper](https://arxiv.org/abs/1408.5882) implementation taken from [Dive into Deep Learning](https://d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-cnn.html).\n",
    "\n",
    "Results:\n",
    "- f1 score   macro avg: 0.69\n",
    "- f1 score  weighted avg: 0.75\n",
    "\n",
    "Note (from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)):\n",
    "- F1 = 2 * (precision * recall) / (precision + recall)\n",
    "- 'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "- 'macro': Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "\n",
    "Model weights:\\\n",
    "No pretrained model weights required.\n",
    "\n",
    "Data:\\\n",
    "Data is downloaded when the notebook is executed from hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c446f4a7-7468-4aab-934c-772366b22457",
   "metadata": {},
   "outputs": [],
   "source": [
    "!module load CUDA/11.3\n",
    "!module load cuDNN/8.2.1.32-CUDA-11.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186cc52-9b12-4594-91e0-d648e5ce185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "\n",
    "import datasets\n",
    "\n",
    "import skmultilearn.model_selection.iterative_stratification\n",
    "\n",
    "import sklearn.metrics\n",
    "import keras.preprocessing.sequence\n",
    "import keras.preprocessing.text\n",
    "\n",
    "import torch\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c639bce4-5efc-46cf-9ac7-5122a32ee46c",
   "metadata": {},
   "source": [
    "## Check if gpus are available  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87976af-2bbc-48eb-aa5a-deb4f9624619",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.utils.collect_env\n",
    "print(torch.version.cuda)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e208d780-4d14-46b1-9bae-9af1f5e9df6b",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ce3d4-3c26-46a2-87d5-0ff740dddf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = datasets.load_dataset(\"go_emotions\", \"simplified\")\n",
    "num_labels = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b42982-90de-4591-adfd-a065219e065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [\n",
    "        emotions.data[\"train\"].table.to_pandas(),\n",
    "        emotions.data[\"validation\"].table.to_pandas(),\n",
    "        emotions.data[\"test\"].table.to_pandas(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dd4dac-62a8-4136-8a33-107c2832a322",
   "metadata": {},
   "source": [
    "### Reduce labels from **27 categories of emotions + neutral** to **emotional + neutral** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a377f4-bae8-4f81-82b0-5702ac619cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = utils.convert_df_labels(df, num_labels)\n",
    "df = utils.remove_ambiguous_data(df, y)\n",
    "y = utils.convert_df_labels(df, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f920f5d1-8283-4396-a876-47b642dc5b71",
   "metadata": {},
   "source": [
    "## Split data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca6c6ad-fcfd-4ac3-a293-debedc50d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanation for iterative stratification of labels http://videolectures.net/ecmlpkdd2011_tsoumakas_stratification/?q=stratification%20multi%20label\n",
    "(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    ") = skmultilearn.model_selection.iterative_stratification.iterative_train_test_split(\n",
    "    df[\"text\"].values.reshape(-1, 1), y, 0.1\n",
    ")\n",
    "X_train = X_train[:, 0]\n",
    "X_test = X_test[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330bc52-da9e-48bd-8b66-65384cc8c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee39f215-5ead-4edf-8f71-06b30d4e0b47",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacb9396-1a1a-4ab3-aca2-52652e9465fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sen)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd39383-661f-48f7-9803-cd2b6d2d11bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_preprocess_text = np.vectorize(preprocess_text)\n",
    "X_train = vectorized_preprocess_text(X_train)\n",
    "X_test = vectorized_preprocess_text(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c5a4ec-0140-48f9-85f3-8c16def3761f",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aae02da-10d1-49ea-adb9-8928b1dd73c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 5000\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, padding=\"post\", maxlen=maxlen)\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test, padding=\"post\", maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65941fb-4c93-4882-8a6b-8a0a7a4fc0a1",
   "metadata": {},
   "source": [
    "## Load [glove text embeddings](https://nlp.stanford.edu/projects/glove/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d16def9-5dd6-4038-a798-853bd96c842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open(\n",
    "    \"/p/project/deepacf/maelstrom/ehlert1/embeddings/glove.6B/glove.6B.100d.txt\",\n",
    "    encoding=\"utf8\",\n",
    ")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype=\"float32\")\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > num_words - 1:\n",
    "        continue\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b9e3d4-3de6-4675-81c6-86a206bd22b1",
   "metadata": {},
   "source": [
    "## Define functions to binarize labels depending on situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b795f7-6b10-459f-a411-6a08c7de4259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_labels_torch(labels):\n",
    "    \"\"\"\n",
    "    returns labels in format [0, 1, 1, 0,.....]\n",
    "    \"\"\"\n",
    "    y_binary = np.zeros(labels.shape[0])\n",
    "    mask = labels[:, -1] == 1\n",
    "    y_binary[mask] = 1\n",
    "    return y_binary\n",
    "\n",
    "\n",
    "def binarize_labels_scikitlearn(labels):\n",
    "    \"\"\"\n",
    "    returns labels in format [[1, 0], [0, 1], [0, 1], [1, 0],.....]\n",
    "    \"\"\"\n",
    "    y_binary = np.zeros((labels.shape[0], 2))\n",
    "    mask = labels[:, -1] == 1\n",
    "    y_binary[mask, 1] = 1\n",
    "    y_binary[np.logical_not(mask), 0] = 1\n",
    "    return y_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30208a2-5aff-4adb-8634-2ec7caa721cc",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e237a8-e7ac-4200-93b1-b520d0a0c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    \"\"\"Compute the number of correct predictions.\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat == y\n",
    "    return float(cmp.sum())\n",
    "\n",
    "\n",
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    \"\"\"Compute the accuracy for a model on a dataset using a GPU.\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    # No. of correct predictions, no. of predictions\n",
    "    metric = Accumulator(2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                # Required for BERT Fine-tuning (to be covered later)\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "\n",
    "class Accumulator:\n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        \"\"\"Defined in :numref:`sec_utils`\"\"\"\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance=5, min_delta=0):\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta  # relative\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss):\n",
    "        if (validation_loss - train_loss) / (validation_loss) > self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.tolerance:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b203833-3b5b-402f-b2c3-84a1e0948c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from https://d2l.ai/chapter_computer-vision/image-augmentation.html\n",
    "\n",
    "\n",
    "def train_batch(net, X, y, loss, trainer, devices):\n",
    "    \"\"\"Train for a minibatch with mutiple GPUs (defined in Chapter 13).\"\"\"\n",
    "    if isinstance(X, list):\n",
    "        # Required for BERT fine-tuning (to be covered later)\n",
    "        X = [x.to(devices[0]) for x in X]\n",
    "    else:\n",
    "        X = X.to(devices[0])\n",
    "    y = y.to(devices[0])\n",
    "    net.train()\n",
    "    trainer.zero_grad()\n",
    "    pred = net(X)\n",
    "    l = loss(pred, y)\n",
    "    l.sum().backward()\n",
    "    trainer.step()\n",
    "    train_loss_sum = l.sum()\n",
    "    train_acc_sum = accuracy(pred, y)\n",
    "    return train_loss_sum, train_acc_sum\n",
    "\n",
    "\n",
    "def train(\n",
    "    net,\n",
    "    train_iter,\n",
    "    test_iter,\n",
    "    loss,\n",
    "    trainer,\n",
    "    num_epochs,\n",
    "    devices=list(range(torch.cuda.device_count())),\n",
    "    early_stopping=False,\n",
    "):\n",
    "    \"\"\"Train a model with mutiple GPUs.\"\"\"\n",
    "    num_batches = len(train_iter)\n",
    "    net = torch.nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    if early_stopping:\n",
    "        _early_stopping = EarlyStopping(tolerance=5, min_delta=0.01)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"starting epoch: {epoch}\")\n",
    "        # Sum of training loss, sum of training accuracy, no. of examples,\n",
    "        # no. of predictions\n",
    "        metric = Accumulator(4)\n",
    "        for i, (features, labels) in enumerate(tqdm(train_iter)):\n",
    "            l, acc = train_batch(net, features, labels, loss, trainer, devices)\n",
    "            metric.add(l, acc, labels.shape[0], labels.numel())\n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "        print(\"train loss: %g, train acc: %g, test_acc: %g\" % (metric[0] / metric[2], metric[1] / metric[3], test_acc))\n",
    "        if early_stopping and _early_stopping.early_stop:\n",
    "            print(\"Early stopping at epoch:\", epoch)\n",
    "            break\n",
    "    print(f\"loss {metric[0] / metric[2]:.3f}, train acc \" f\"{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31dc14-b3bd-420f-a720-35e32bb54ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels, **kwargs):\n",
    "        super(TextCNN, self).__init__(**kwargs)\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_size)\n",
    "        # The embedding layer not to be trained\n",
    "        self.constant_embedding = torch.nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.decoder = torch.nn.Linear(sum(num_channels), 2)\n",
    "        # The max-over-time pooling layer has no parameters, so this instance\n",
    "        # can be shared\n",
    "        self.pool = torch.nn.AdaptiveAvgPool1d(1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # Create multiple one-dimensional convolutional layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(torch.nn.Conv1d(2 * embed_size, c, k))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Concatenate two embedding layer outputs with shape (batch size, no.\n",
    "        # of tokens, token vector dimension) along vectors\n",
    "        embeddings = torch.cat((self.embedding(inputs), self.constant_embedding(inputs)), dim=2)\n",
    "        # Per the input format of one-dimensional convolutional layers,\n",
    "        # rearrange the tensor so that the second dimension stores channels\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        # For each one-dimensional convolutional layer, after max-over-time\n",
    "        # pooling, a tensor of shape (batch size, no. of channels, 1) is\n",
    "        # obtained. Remove the last dimension and concatenate along channels\n",
    "        encoding = torch.cat(\n",
    "            [torch.squeeze(self.relu(self.pool(conv(embeddings))), dim=-1) for conv in self.convs],\n",
    "            dim=1,\n",
    "        )\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def try_all_gpus():\n",
    "    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\n",
    "\n",
    "    Defined in :numref:`sec_use_gpu`\"\"\"\n",
    "    devices = [torch.device(f\"cuda:{i}\") for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device(\"cpu\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efa5ac4-cacd-452c-b67b-ebf1b2bbdf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.x_train = torch.tensor(X_train, dtype=torch.long)\n",
    "        self.y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_train[idx], self.y_train[idx]\n",
    "\n",
    "\n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"Construct a PyTorch data iterator.\n",
    "\n",
    "    Defined in :numref:`sec_linear_concise`\"\"\"\n",
    "    dataset = torch.utils.data.TensorDataset(*data_arrays)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "\n",
    "def load_data(batch_size, train_data, train_features, test_data, test_features):\n",
    "    train_iter = torch.utils.data.DataLoader(\n",
    "        MyDataset(train_data, train_features),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    test_iter = torch.utils.data.DataLoader(\n",
    "        MyDataset(test_data, test_features),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8d5f91-7662-426c-975f-75c828161e51",
   "metadata": {},
   "source": [
    "## Pre-load embeddings of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692cef6-00a6-4e54-aa93-91b611b23512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) in (torch.nn.Linear, torch.nn.Conv1d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f15db24-0d00-42c9-8aa9-c7820cfe3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_labels = 2\n",
    "train_iter, test_iter = load_data(\n",
    "    batch_size,\n",
    "    X_train,\n",
    "    binarize_labels_torch(y_train),\n",
    "    X_test,\n",
    "    binarize_labels_torch(y_test),\n",
    ")\n",
    "\n",
    "embed_size = 100\n",
    "kernel_sizes = [3, 4, 5]\n",
    "nums_channels = [100, 100, 100]\n",
    "net = TextCNN(num_words, embed_size, kernel_sizes, nums_channels)\n",
    "\n",
    "net.apply(init_weights)\n",
    "\n",
    "embeds = torch.Tensor(embedding_matrix)\n",
    "net.embedding.weight.data.copy_(embeds)\n",
    "net.constant_embedding.weight.data.copy_(embeds)\n",
    "net.constant_embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fea773-5f30-493e-b02b-b04b8c3f004a",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d84e3-d001-498e-bfda-0b7afecf55ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 3  # model seems to overfit after 3 epochs\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "loss = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "train(\n",
    "    net,\n",
    "    train_iter,\n",
    "    test_iter,\n",
    "    loss,\n",
    "    trainer,\n",
    "    num_epochs,\n",
    "    try_all_gpus(),\n",
    "    early_stopping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a376c3e6-a85b-4502-b03d-5750d3f939e0",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b62cf25-ad72-4b62-a209-6cfda4db6263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to evaluate mode\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b586dd47-a220-46dd-876c-60601497b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for i, j in iter(test_iter):\n",
    "        if torch.cuda.is_available():\n",
    "            x = torch.as_tensor(i, device=torch.device(\"cuda\"))\n",
    "        else:\n",
    "            x = i\n",
    "        out_data = net(x).cpu().detach().numpy()\n",
    "        preds.extend(out_data)\n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d787706-43d1-47e1-8b5f-2073e567c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scikitlearn = binarize_labels_scikitlearn(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a8186-dbb7-446a-8038-b8362dcd45c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate = dict()\n",
    "true_positive_rate = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(num_labels):\n",
    "    (\n",
    "        false_positive_rate[i],\n",
    "        true_positive_rate[i],\n",
    "        _,\n",
    "    ) = sklearn.metrics.roc_curve(y_test_scikitlearn[:, i], preds[:, i])\n",
    "    roc_auc[i] = sklearn.metrics.auc(false_positive_rate[i], true_positive_rate[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6851e53b-03b6-42a6-b0f1-5c31b6d28806",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "for i in range(num_labels):\n",
    "    plt.plot(\n",
    "        false_positive_rate[i],\n",
    "        true_positive_rate[i],\n",
    "        lw=lw,\n",
    "        label=\"ROC curve (area = %0.2f) for %i\" % (roc_auc[i], i),\n",
    "    )\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic example\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4dd057-fff4-4569-9bdc-f20977633c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    sklearn.metrics.classification_report(\n",
    "        y_test_scikitlearn.argmax(-1),\n",
    "        preds.argmax(-1),\n",
    "        target_names=[\"emotional\", \"neutral\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4a327-41b5-442d-ae51-f5210842239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sklearn.metrics.confusion_matrix(y_test_scikitlearn.argmax(-1), preds.argmax(-1))\n",
    "disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"emotional\", \"neutral\"])\n",
    "disp.plot()\n",
    "ax = plt.gca()\n",
    "ax.tick_params(axis=\"x\", labelrotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e293706c-5bf5-4781-b8db-19f47bf84683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_twitter",
   "language": "python",
   "name": "twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
