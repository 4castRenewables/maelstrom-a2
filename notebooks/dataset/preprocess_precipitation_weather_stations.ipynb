{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data from weather stations as pandas dataframe\n",
    "- Downloaded files on measured hourly precipitation from [CEDA](https://data.ceda.ac.uk/badc/ukmo-midas-open/data/uk-hourly-rain-obs/dataset-version-202207). \n",
    "- Here, saving files to dataframe and removing duplicates within single file. \n",
    "- Files are concatenated in multindex data frame with latitude and longitude of the weather station as the two indices.\n",
    "\n",
    "Info on weather station data:\n",
    "- `prcp_amt` gives hourly observed total rainfal in mm to nearest 0.2mm\n",
    "- `prcp_dur` gives duration of rainfall within that hour to nearest 6min (rarely noted in files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows update of external libraries without need to reload package\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import tqdm\n",
    "import a2.utils\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "start_dir = \"/home/kristian/Projects/a2/data/weather_stations/dap.ceda.ac.uk/\"\n",
    "pattern = r\"*/(?!capability).csv\"\n",
    "pattern = r\"*.csv\"\n",
    "\n",
    "for dir, _, _ in os.walk(start_dir):\n",
    "    files.extend(glob.glob(os.path.join(dir, pattern)))\n",
    "files = [f for f in files if \"capability.csv\" not in f and \"station-metadata.csv\" not in f and \"_qcv-1_\" in f]\n",
    "print(f\"{len(files)} fiels in total\")\n",
    "files[182]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_arrays_equal(array1, array2):\n",
    "    if not np.array_equal(array1, array2):\n",
    "        for a1, a2 in zip(array1, array2):\n",
    "            if a1 != a2:\n",
    "                print(f\"{a1=} != {a2=}\")\n",
    "        raise ValueError(f\"{array1=} not same as expected: {array2=}!\")\n",
    "\n",
    "\n",
    "def location_from_header(header):\n",
    "    s = \"\".join(header)\n",
    "    r = re.findall(\"location,G,(-?\\d+.\\d+),(-?\\d+.\\d+)\", s)\n",
    "    if np.shape(r) != (1, 2):\n",
    "        raise ValueError(f\"Location not correctly extracted from header: {s=}, extracted: {r=}\")\n",
    "    latitude = float(r[0][0])\n",
    "    longitude = float(r[0][1])\n",
    "    return latitude, longitude\n",
    "\n",
    "\n",
    "def station_name_from_header(header):\n",
    "    s = \"\".join(header)\n",
    "    station = re.findall(\"observation_station,G,(.+)\\n\", s)[0]\n",
    "    county = re.findall(\"historic_county_name,G,(.+)\\n\", s)[0]\n",
    "    return county + \"_\" + station\n",
    "\n",
    "\n",
    "def load_single_csv(\n",
    "    filename,\n",
    "    only_hourly=True,\n",
    "    check_duplicate=True,\n",
    "    check_domain=True,\n",
    "    keep_max_tp=True,\n",
    "):\n",
    "    # For description of columns see https://artefacts.ceda.ac.uk/badc_datadocs/ukmo-midas/RH_Table.html\n",
    "    # Or header of the csv files\n",
    "    # Documentation for CEDA can be found here http://cedadocs.ceda.ac.uk/1492/\n",
    "    df = pd.read_csv(filename, skiprows=61)\n",
    "    if df[\"ob_end_time\"].isnull().sum():\n",
    "        raise ValueError(f\"Nan time values in {filename}!\")\n",
    "    column_names = np.array(\n",
    "        [\n",
    "            \"ob_end_time\",\n",
    "            \"id\",\n",
    "            \"id_type\",\n",
    "            \"ob_hour_count\",\n",
    "            \"version_num\",\n",
    "            \"met_domain_name\",\n",
    "            \"src_id\",\n",
    "            \"rec_st_ind\",\n",
    "            \"prcp_amt\",\n",
    "            \"prcp_dur\",\n",
    "            \"prcp_amt_q\",\n",
    "            \"prcp_dur_q\",\n",
    "            \"prcp_amt_j\",\n",
    "            \"meto_stmp_time\",\n",
    "            \"midas_stmp_etime\",\n",
    "        ],\n",
    "        dtype=object,\n",
    "    )\n",
    "    assert_arrays_equal(column_names, df.columns.values)\n",
    "    if df[\"ob_end_time\"].values[-1] == \"end data\":\n",
    "        df.drop(df.tail(1).index, inplace=True)\n",
    "    if not np.all(df[\"id_type\"].values == \"RAIN\"):\n",
    "        raise ValueError(\n",
    "            f\"{filename}:\\nUnexpected value in df['id_type']==RAIN, but found {[x for x in df['id_type'].values if x != 'RAIN']}\"\n",
    "        )\n",
    "    df[\"ob_end_time\"] = pd.to_datetime(df[\"ob_end_time\"])\n",
    "    df[\"meto_stmp_time\"] = pd.to_datetime(df[\"meto_stmp_time\"])\n",
    "    # dropping rows when neither duration of precipitation `prcp_dur` nor amount of precipitation `prcp_amt` is available\n",
    "    df = df.drop(df[df[\"prcp_dur\"].isnull() & df[\"prcp_amt\"].isnull()].index).reset_index(drop=True)\n",
    "    if only_hourly:\n",
    "        df = df.loc[df[\"ob_hour_count\"] == 1].reset_index(drop=True)\n",
    "    if check_domain:\n",
    "        mask = df[\"met_domain_name\"] != \"SREW\"\n",
    "        counts_non_srew = mask.sum()\n",
    "        if counts_non_srew:\n",
    "            # Note, SSER or SAMOS also possible but shouldn't be relevant for our domain (hourly rain water: SREW)\n",
    "            mask_not = ~mask\n",
    "            # logging.info(f\"Removing {counts_non_srew}/{df.shape[0]} non-SREW domains (found: {np.unique(df['met_domain_name'].loc[mask])}, time: {df['ob_end_time'].loc[mask].min()}-{df['ob_end_time'].loc[mask].max()}) from dataframe!\")\n",
    "            df = df.loc[mask_not].reset_index(drop=True)\n",
    "    if check_duplicate:\n",
    "        # drop measurements of different devices (marked by different `id` which are stored at different times possibly `meto_stmp_time`/`midas_stmp_etime`) but with same results\n",
    "        df = df.drop_duplicates(\n",
    "            subset=[x for x in df.columns.values if x not in [\"id\", \"meto_stmp_time\", \"midas_stmp_etime\"]],\n",
    "            keep=\"first\",\n",
    "        ).reset_index(drop=True)\n",
    "        # check if only difference is due to State Indicators, if case only retain 1011\n",
    "        if (\n",
    "            df.loc[df.duplicated(subset=[\"ob_end_time\", \"ob_hour_count\"])].shape[0]\n",
    "            and not df.loc[\n",
    "                df.duplicated(\n",
    "                    subset=[\"ob_end_time\", \"ob_hour_count\", \"rec_st_ind\"],\n",
    "                    keep=False,\n",
    "                )\n",
    "            ].shape[0]\n",
    "        ):\n",
    "            df = df.drop(\n",
    "                df[\n",
    "                    df.duplicated(subset=[\"ob_end_time\", \"ob_hour_count\"], keep=False) & (df[\"rec_st_ind\"] != 1011)\n",
    "                ].index\n",
    "            ).reset_index(drop=True)\n",
    "        # check if duplicates only occure due to same `ob_end_time` but varying 'id', 'meto_stmp_time', 'midas_stmp_etime', if case retain first occurence\n",
    "        # sort values by prcp_amt (nan-values will come first), drop duplicate values that do not agree in 'id', 'meto_stmp_time', 'midas_stmp_etime' by picking the highest value in `prcp_amt`\n",
    "        if keep_max_tp:\n",
    "            df = df.sort_values(\"prcp_amt\", na_position=\"first\")\n",
    "            df = df.drop(\n",
    "                df[\n",
    "                    ~df.duplicated(\n",
    "                        subset=[x for x in df.columns.values if x not in [\"id\", \"meto_stmp_time\", \"midas_stmp_etime\"]],\n",
    "                        keep=\"last\",\n",
    "                    )\n",
    "                    & df.duplicated(subset=[\"ob_end_time\", \"ob_hour_count\"], keep=\"last\")\n",
    "                ].index\n",
    "            ).reset_index(drop=True)\n",
    "        if df.loc[df.duplicated(subset=[\"ob_end_time\", \"ob_hour_count\"])].shape[0]:\n",
    "            raise ValueError(\n",
    "                f\"{filename}:\\nDuplicate columns with the same `ob_end_time` and `ob_hour_count`: {df.loc[df.duplicated(subset=['ob_end_time', 'ob_hour_count'], keep=False)]}\"\n",
    "            )\n",
    "    # only retain data with status code 1011 -> \"Normal ingestion of observation at creation\"\n",
    "    if not df[\"rec_st_ind\"].loc[df[\"rec_st_ind\"] != 1011].all():\n",
    "        # see https://dap.ceda.ac.uk/badc/ukmo-midas/metadata/doc/state_indicators.html\n",
    "        raise ValueError(\n",
    "            f\"{filename}:\\nRecords found that do not have QC=1, found State indicators{df['rec_st_ind'].value_counts()}\"\n",
    "        )\n",
    "    df.attrs[\"header\"] = a2.utils.file_handling.get_header(filename, 60)\n",
    "    df.attrs[\"latitude\"], df.attrs[\"longitude\"] = location_from_header(df.attrs[\"header\"])\n",
    "    df.attrs[\"station_name\"] = station_name_from_header(df.attrs[\"header\"])\n",
    "    df[\"latitude\"] = df.attrs[\"latitude\"]\n",
    "    df[\"longitude\"] = df.attrs[\"longitude\"]\n",
    "    df[\"station_name\"] = df.attrs[\"station_name\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv_multi_index(filename, only_hourly=True, check_duplicate=True, check_domain=True):\n",
    "    df = load_single_csv(\n",
    "        filename,\n",
    "        only_hourly=only_hourly,\n",
    "        check_duplicate=check_duplicate,\n",
    "        check_domain=check_domain,\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def unique_coordinates(df, coordinates=None):\n",
    "    if coordinates is None:\n",
    "        coordinates = [\"latitude\", \"longitude\"]\n",
    "    return np.unique(np.array([df[x].values for x in coordinates]), axis=1)\n",
    "\n",
    "\n",
    "def load_all_files(files):\n",
    "    loaded = [load_csv_multi_index(f) for f in tqdm.tqdm(files)]\n",
    "    df = pd.concat(loaded).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weather_stations(filename):\n",
    "    df = pd.read_csv(\n",
    "        filename,\n",
    "        # usecols=[\"latitude\", \"longitude\", \"ob_end_time\", \"prcp_amt\"],\n",
    "        dtype={\"latitude\": float, \"longitude\": float, \"prcp_amt\": float},\n",
    "        parse_dates=[\"ob_end_time\"],\n",
    "    )\n",
    "    df = df.set_index([\"latitude\", \"longitude\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_all_files(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_duplicates_coord_based(df):\n",
    "    for latitude, longitude in zip(*unique_coordinates(df)):\n",
    "        coordinate_mask = (df.latitude == latitude) & (df.longitude == longitude)\n",
    "        mask = coordinate_mask & df.loc[coordinate_mask].duplicated(subset=[\"ob_end_time\"], keep=False)\n",
    "        duplicates = df.loc[mask]\n",
    "        if duplicates.shape[0]:\n",
    "            print(f\"{latitude=}, {longitude=}\")\n",
    "            print((duplicates).sort_values(\"ob_end_time\"))\n",
    "\n",
    "\n",
    "def drop_duplicates_same_location(df):\n",
    "    # duplicates include entries where \"prcp_amt\", \"prcp_dur\" differ -> retain larger value\n",
    "    for latitude, longitude in zip(*unique_coordinates(df)):\n",
    "        coordinate_mask = (df.latitude == latitude) & (df.longitude == longitude)\n",
    "        df = df.sort_values(\"prcp_amt\", na_position=\"first\")\n",
    "        df = df.drop(\n",
    "            df.loc[coordinate_mask][df.loc[coordinate_mask].duplicated(subset=[\"ob_end_time\"], keep=\"last\")].index\n",
    "        ).reset_index(drop=True)\n",
    "    print(f\"{df.loc[df.duplicated(subset=['ob_end_time'], keep=False)].sort_values('ob_end_time')=}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = drop_duplicates_same_location(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique.to_csv(\"../../data/weather_stations/weather_stations_hourly_rainfall_uk.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -tlrh '../../data/weather_stations/weather_stations_hourly_rainfall_uk_2017-2020_reduced.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_weather_stations(\n",
    "    \"../../data/weather_stations/weather_stations_hourly_rainfall_uk.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_duplicates_coord_based(df):\n",
    "    for latitude, longitude in zip(*unique_coordinates(df)):\n",
    "        coordinate_mask = (df.latitude == latitude) & (df.longitude == longitude)\n",
    "        mask = coordinate_mask & df.loc[coordinate_mask].duplicated(subset=[\"ob_end_time\"], keep=False)\n",
    "        duplicates = df.loc[mask]\n",
    "        if duplicates.shape[0]:\n",
    "            print(f\"{latitude=}, {longitude=}\")\n",
    "            print((duplicates).sort_values(\"ob_end_time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_duplicates_coord_based(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1720 = load_weather_stations(\"../../data/weather_stations/weather_stations_hourly_rainfall_uk_2017-2020_reduced.csv\")\n",
    "df_1720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_duplicates_coord_based(df_1720.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1720 = df.loc[\n",
    "    (df[\"ob_end_time\"] > np.datetime64(\"2017-01-01 00:00:00\"))\n",
    "    & (df[\"ob_end_time\"] < np.datetime64(\"2021-01-01 00:00:00\"))\n",
    "].reset_index(drop=True)\n",
    "df_1720.to_csv(\"../../data/weather_stations/weather_stations_hourly_rainfall_uk_2017-2020_reduced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_duplicates_coord_based(df_1720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemsby = df.loc[df[\"station_name\"].str.contains(\"norfolk_hemsby\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"station_name\"] == \"norfolk_hemsby-trial\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemsby.loc[\n",
    "    hemsby.duplicated(subset=[\"ob_end_time\"], keep=False)\n",
    "    & ~hemsby.duplicated(subset=[\"ob_end_time\", \"prcp_amt\", \"prcp_dur\"], keep=False)\n",
    "].sort_values(\"ob_end_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_same_site(df):\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df.drop(df[df.duplicated(subset=[\"ob_end_time\", \"prcp_amt\", \"prcp_dur\"], keep=\"first\")].index).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    df = df.sort_values(\"ob_end_time\").reset_index(drop=True)\n",
    "    duplicate_times = df.loc[df.duplicated(subset=[\"ob_end_time\"], keep=False)]\n",
    "    if duplicate_times.shape[0]:\n",
    "        # keep maximum values\n",
    "        logging.info(\"Have {duplicate_times.shape[0]} remaining duplicates, keep max value.\")\n",
    "        df = df.sort_values(\"prcp_amt\", na_position=\"first\")\n",
    "        df = df.drop(df[df.duplicated(subset=[\"ob_end_time\"], keep=\"last\")].index).reset_index(drop=True)\n",
    "    duplicate_times = df.loc[df.duplicated(subset=[\"ob_end_time\"], keep=False)]\n",
    "    print(f\"{duplicate_times.shape[0]} values are duplicates\")\n",
    "    return df.sort_values(\"ob_end_time\")\n",
    "\n",
    "\n",
    "hemsby_r = remove_duplicates_same_site(hemsby.copy())\n",
    "hemsby_r.loc[hemsby_r[\"ob_end_time\"] == np.datetime64(\"2000-07-31 04:00:00\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemsby.loc[hemsby.duplicated(subset=[\"ob_end_time\"], keep=False)].sort_values(\"ob_end_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemsby.loc[hemsby.duplicated(subset=[\"ob_end_time\", \"prcp_amt\", \"prcp_dur\"], keep=False)].sort_values(\"ob_end_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../data/weather_stations/weather_stations_hourly_rainfall_uk.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"prcp_amt\"].plot.hist(bins=np.linspace(0, 10, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('a2-laF_Cm_L-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "a06658bfc983828e336131948d402e02953311214b959ca171c834c9d9b531a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
