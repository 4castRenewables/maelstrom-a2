{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf6ab4b-56bf-4253-a059-afca293661a5",
   "metadata": {},
   "source": [
    "# Analyze first dataset; emojis and sun/rain keywords for uk in 2020 \n",
    "\n",
    "First overview analysis of tweets looking at:\n",
    "- frequency of tweeting, time of day\n",
    "- word clouds\n",
    "- occurence of keywords in tweets\n",
    "\n",
    "In addition, attempt at tokenizing tweets to obtain more informed word clouds and analysis essence of tweets.\n",
    "\n",
    "Results include:\n",
    "- generate cleaned file with tokenized tweets 'df_all_tokenized.csv'\n",
    "    - removed urls, hashtags #TEXT, at sign @TEXT \n",
    "    - dealing with emojis is non-trivial, included string manipulation to replace emojis with their name or include emoji symbols\n",
    "    - obvious bots contribute ~100_000 tweets (20%), remove them for now as expect strong bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b4524-3e91-465e-bf63-4d2321c87f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows update of external libraries without need to reload package\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fefc20b-9e96-41e8-b646-269133bba1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import pathlib\n",
    "import logging\n",
    "import json\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "import wget\n",
    "\n",
    "import wordcloud\n",
    "import tweepy\n",
    "import a2.twitter.downloader\n",
    "import a2.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f894611-1df1-4111-b7c5-7c3f14b8f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "wget.download(\n",
    "    \"https://www.wfonts.com/download/data/2016/04/23/symbola/symbola.zip\",\n",
    "    out=\"/tmp/\",\n",
    ")\n",
    "with zipfile.ZipFile(\"/tmp/symbola.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"fonts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110628c9-539e-4655-9a4f-12591cb0167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../data/tweets/gb_2020_rain_sun_vocab_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef3873-3d63-49e8-b135-b607a7e26555",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../data/tweets/gb_2020_rain_sun_vocab_emojis/\"\n",
    "font_path = \"../src/a2/data/font/Symbola.ttf\"  # required to render emojis\n",
    "figure_path = pathlib.Path(\"../figures/data/gb_2020_rain_sun_vocab_emojis/\")\n",
    "if not os.path.exists(figure_path):\n",
    "    os.makedirs(figure_path)\n",
    "path = os.path.abspath(filepath)  # use your path\n",
    "all_files = glob.glob(os.path.join(path, \"tweets_2020*.json\"))\n",
    "\n",
    "a2.dataset.load_dataset.load_tweets_dataset_from_jsons(all_files)\n",
    "\n",
    "ds[\"created_at\"] = ([\"index\"], pd.to_datetime(ds.created_at).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc65b06-9c2d-4a19-86f3-dcdcff880747",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -2 /home/kristian/Projects/a2/data/tweets/gb_2020_rain_sun_vocab_emojis/tweets_2020_08.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436d3410-6f52-4860-9b6e-c7335b58895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = \"🏔️ OR 🏔️ OR ☀️ OR ☀️ OR 🌞 OR ⛅ OR ⛈️ OR ⛈️ OR 🌤️ OR 🌤️ OR 🌥️ OR 🌥️ OR 🌦️ OR 🌦️ OR 🌧️ OR 🌧️ OR 🌨️ OR 🌨️ OR 🌩️ OR 🌩️ OR ☔ OR ⛄ OR blizzard OR cloudburst OR downpour OR drizzle OR flash flood OR flood OR flood stage OR forecast OR freezing rain OR hail OR ice storm OR lightning OR precipitation OR rain OR rain gauge OR rain shadow OR rainbands OR rain shower OR snow OR snow shower OR snowstorm OR sun OR sunny OR thunder OR thunderstorm\"\n",
    "keywords = header.split(\" OR \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4083e458-a4fc-4cbb-976c-cb8684b7115e",
   "metadata": {},
   "source": [
    "## Sample tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad59f4-a929-440e-a1b5-42d68e24aa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2.dataset.utils_dataset.print_tweet_sample(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd38271-4721-4aee-89c2-c2c7a509b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = a2.dataset.utils_dataset.print_tweet_authors(ds, n_sample=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b993c6-bdca-481d-a024-1cbc5ba2f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = a2.dataset.utils_dataset.print_tweet_authors(ds, n_sample=5, authors=authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c0556-39a9-472d-b7ae-30d008067582",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_no_bots = ds.where(\n",
    "    (ds.source == \"Twitter for iPhone\")\n",
    "    | (ds.source == \"Twitter for Android\")\n",
    "    | (ds.source == \"Instagram\")\n",
    "    | (ds.source == \"Twitter for iPad\")\n",
    "    | (ds.source == \"Twitter Web Client\"),\n",
    "    drop=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f521ba93-0df3-44e4-a3ea-94d21c179613",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_no_bots = a2.dataset.utils_dataset.print_tweet_authors(ds_no_bots, n_sample=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b333ec5-751c-4cf7-a553-2c4450ce3759",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Look at time: created_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0a6e3-72c4-4fa3-b7de-ddbbc06fe7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.groupby(\"created_at.minute\").count().plot.scatter(x=\"minute\", y=\"author_id\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a100ec-ecf8-423d-b7c0-766d14add80c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Significant influx of new tweets at full hour (bots?!) -> look at source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe8ff6-c895-4ee4-903b-1ed0a03f6079",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_full_hour = ds.where(ds.created_at.dt.minute == 0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed20c3f-0621-4e6e-b933-a72340db4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_full_hour.groupby(\"source\").count().plot.scatter(y=\"source\", x=\"tweet_id\", size=20)\n",
    "ax = plt.gca()\n",
    "ax.tick_params(axis=\"x\", labelrotation=0)\n",
    "ax.set_xlabel(\"source\")\n",
    "ax.set_ylabel(\"count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adae489-34f3-4aad-80d3-54737c5f71ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.where(ds.source.str.contains(r\"MeteoWare Plus+\"), drop=True).groupby(\"created_at.minute\").count().plot.scatter(\n",
    "    x=\"minute\", y=\"author_id\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc78a283-4883-436b-92b0-2fdd38899957",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.where(ds.source.str.contains(r\"pywws\"), drop=True).groupby(\"created_at.minute\").count().plot.scatter(\n",
    "    x=\"minute\", y=\"author_id\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350063a0-734e-421a-b8d9-7815bbeb89a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.where(ds.source.str.contains(r\"Sandaysoft Cumulus\"), drop=True).groupby(\"created_at.minute\").count().plot.scatter(\n",
    "    x=\"minute\", y=\"author_id\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819462fa-d270-41d2-9bb1-28509f9a89e5",
   "metadata": {},
   "source": [
    "## Time distribution of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14634fda-36e9-41e0-9761-44484e1d69cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.groupby(\"created_at.hour\").count().plot.scatter(x=\"hour\", y=\"tweet_id\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0172d210-7f39-4387-936f-878a840b8e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.groupby(\"created_at.dayofyear\").count().plot.scatter(x=\"dayofyear\", y=\"tweet_id\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9dba86-1f32-40f7-b1a6-c5746069d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.groupby(\"created_at.month\").count().plot.scatter(x=\"month\", y=\"tweet_id\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77409b49-8e5d-4369-aac0-bf3f6b525259",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.groupby(\"author_id\").count()[\"tweet_id\"].plot.hist(bins=100)\n",
    "ax = plt.gca()\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"occurence of author_id\")\n",
    "ax.set_ylabel(\"count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ede157f-1abf-4e42-8617-5e107959cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.groupby(\"source\").count().plot.scatter(y=\"source\", x=\"tweet_id\", size=20)\n",
    "ax = plt.gca()\n",
    "ax.tick_params(axis=\"x\", labelrotation=0)\n",
    "ax.set_xlabel(\"source\")\n",
    "ax.set_ylabel(\"count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766a910-ba0f-4eba-9feb-6a98541cafc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Most active individual users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47cef85-e112-4263-ba2f-e4aa27eaf8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity = ds.groupby(\"author_id\").count().sortby(\"tweet_id\", ascending=False)\n",
    "user_activity_sorted_by_activity = activity[\"author_id\"]\n",
    "number_of_tweets = activity[\"tweet_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2af6db-f86d-436f-b1eb-15b8bef17693",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = number_of_tweets > 2000\n",
    "most_active_users = user_activity_sorted_by_activity[mask].values\n",
    "for user_id, n_tweets in zip(most_active_users, number_of_tweets[mask].values):\n",
    "    user = a2.data_manipulation.twitter.downloader.get_user_from_userid(user_id)[\"data\"]\n",
    "    print(\n",
    "        f'{user[\"name\"]}, {user_id}, @{user[\"username\"]}, {user[\"location\"] if \"location\" in user else \"?\"} --> {n_tweets} tweets'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d9f774-a1c3-4192-9e97-f86ba4350930",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.where(ds.author_id.isin(3029396645), drop=True).groupby(\"created_at.minute\").count().plot.scatter(\n",
    "    x=\"minute\", y=\"author_id\", ax=plt.gca()\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603291ec-b73c-43ae-a7aa-b0dfc8ac0b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_id in most_active_users:\n",
    "    print(\n",
    "        np.unique(ds.where(ds.author_id == user_id, drop=True).author_id),\n",
    "        user_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281cd158-a4e7-4748-afb4-18754bf91eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"font.size\"] = 8\n",
    "plt.rcParams[\"lines.markersize\"] = 2\n",
    "axes = []\n",
    "n_variables = 4\n",
    "n_x = len(most_active_users)\n",
    "n_y = n_variables\n",
    "fig, axs = plt.subplots(n_x, n_y, figsize=(n_y * 2, n_x * 2), constrained_layout=True)\n",
    "for i, user_id in enumerate(most_active_users):\n",
    "    ax = axs[i, 0]\n",
    "    ds.where(ds.author_id == user_id, drop=True).groupby(\"created_at.minute\").count().plot.scatter(\n",
    "        x=\"minute\", y=\"tweet_id\", ax=ax, label=user_id\n",
    "    )\n",
    "    ax.legend()\n",
    "    ax = axs[i, 1]\n",
    "    ds.where(ds.author_id == user_id, drop=True).groupby(\"created_at.hour\").count().plot.scatter(\n",
    "        x=\"hour\", y=\"tweet_id\", ax=ax\n",
    "    )\n",
    "    ax = axs[i, 2]\n",
    "    ds.where(ds.author_id == user_id, drop=True).groupby(\"created_at.dayofyear\").count().plot.scatter(\n",
    "        x=\"dayofyear\", y=\"tweet_id\", ax=ax\n",
    "    )\n",
    "    ax = axs[i, 3]\n",
    "    ds.where(ds.author_id == user_id, drop=True).groupby(\"created_at.month\").count().plot.scatter(\n",
    "        x=\"month\", y=\"tweet_id\", ax=ax\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a4ab1-a6f9-4861-910a-e9122ad84019",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Look at text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd2770f-4ae2-46b0-97af-99f5ff148208",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.where((ds.author_id == 2411260615) & (ds.created_at.dt.month == 4), drop=True).groupby(\n",
    "    \"created_at.dayofyear\"\n",
    ").count().plot.scatter(x=\"dayofyear\", y=\"tweet_id\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e0536f-40d8-4b98-9351-6ab5e60ca18e",
   "metadata": {},
   "source": [
    "## Many tweets in April seem to be duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c6879-dbc3-41dc-817a-ccc099ce362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ds.where(\n",
    "    (ds.author_id == 2411260615) & (ds.created_at.dt.dayofyear > 95) & (ds.created_at.dt.dayofyear < 120),\n",
    "    drop=True,\n",
    ").text.values\n",
    "time = ds.where(\n",
    "    (ds.author_id == 2411260615) & (ds.created_at.dt.dayofyear > 95) & (ds.created_at.dt.dayofyear < 120),\n",
    "    drop=True,\n",
    ").created_at.values\n",
    "mask = np.argsort(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2c2bb7-509b-4ddc-996e-bd9457245fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, index1 = np.unique(ds[\"text\"], return_index=True)\n",
    "_, index2 = np.unique(ds[\"created_at\"], return_index=True)\n",
    "mask = np.intersect1d(index1, index2)\n",
    "print(\n",
    "    f\"total tweets: {ds.tweet_id.shape[0]}, unique tweets & times {mask.shape[0]}, unique number of texts {index1.shape[0]}, unique number of creation times {index2.shape[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72b488-e7a8-424c-a0f3-3bc6ed71a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_unique = ds.sel(index=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662d2f2-10bc-4e51-a69a-2522882a4584",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize unique dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96920ce-f2ba-4114-b70f-75a4aa8aa1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ds_unique\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"font.size\"] = 8\n",
    "plt.rcParams[\"lines.markersize\"] = 2\n",
    "axes = []\n",
    "n_variables = 4\n",
    "n_x = len(most_active_users)\n",
    "n_y = n_variables\n",
    "fig, axs = plt.subplots(n_x, n_y, figsize=(n_y * 2, n_x * 2), constrained_layout=True)\n",
    "for i, user_id in enumerate(most_active_users):\n",
    "    ax = axs[i, 0]\n",
    "    d.where(d.author_id == user_id, drop=True).groupby(\"created_at.minute\").count().plot.scatter(\n",
    "        x=\"minute\", y=\"tweet_id\", ax=ax, label=user_id\n",
    "    )\n",
    "    ax.legend()\n",
    "    ax = axs[i, 1]\n",
    "    d.where(d.author_id == user_id, drop=True).groupby(\"created_at.hour\").count().plot.scatter(\n",
    "        x=\"hour\", y=\"tweet_id\", ax=ax\n",
    "    )\n",
    "    ax = axs[i, 2]\n",
    "    d.where(d.author_id == user_id, drop=True).groupby(\"created_at.dayofyear\").count().plot.scatter(\n",
    "        x=\"dayofyear\", y=\"tweet_id\", ax=ax\n",
    "    )\n",
    "    ax = axs[i, 3]\n",
    "    d.where(d.author_id == user_id, drop=True).groupby(\"created_at.month\").count().plot.scatter(\n",
    "        x=\"month\", y=\"tweet_id\", ax=ax\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab31cfc-0b83-48f7-b49b-fb095d180d1c",
   "metadata": {},
   "source": [
    "## Remove three main sources for weather bots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e50ec9-547e-496f-877f-d41fe2d8ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_red = ds_unique.where(\n",
    "    ~ds_unique.source.str.contains(r\"pywws|MeteoWare Plus+|Sandaysoft Cumulus\"),\n",
    "    drop=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee2ce18-d582-49b7-bf5f-00e6d6be4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ds_red\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"font.size\"] = 8\n",
    "plt.rcParams[\"lines.markersize\"] = 2\n",
    "axes = []\n",
    "n_variables = 4\n",
    "n_x = 2\n",
    "n_y = n_variables\n",
    "users = [d.author_id]\n",
    "fig, axs = plt.subplots(n_x, n_y, figsize=(n_y * 2, n_x * 2), constrained_layout=True)\n",
    "for i, user_id in enumerate(users):\n",
    "    ax = axs[i, 0]\n",
    "    d.where(d.author_id == d.author_id, drop=True).groupby(\"created_at.minute\").count().plot.scatter(\n",
    "        x=\"minute\", y=\"tweet_id\", ax=ax, label=None\n",
    "    )\n",
    "    ax.legend()\n",
    "    ax = axs[i, 1]\n",
    "    d.where(d.author_id == user_id, drop=True).groupby(\"created_at.hour\").count().plot.scatter(\n",
    "        x=\"hour\", y=\"tweet_id\", ax=ax\n",
    "    )\n",
    "    ax = axs[i, 2]\n",
    "    d.where(d.author_id == user_id, drop=True).groupby(\"created_at.dayofyear\").count().plot.scatter(\n",
    "        x=\"dayofyear\", y=\"tweet_id\", ax=ax\n",
    "    )\n",
    "    ax = axs[i, 3]\n",
    "    d.where(d.author_id == user_id, drop=True).groupby(\"created_at.month\").count().plot.scatter(\n",
    "        x=\"month\", y=\"tweet_id\", ax=ax\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a238f3-f9ef-424f-9867-b0e980fa2847",
   "metadata": {},
   "source": [
    "## most popular days for tweeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888f60d-9593-4db7-b935-f47348f8dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_popular_days = ds.groupby(\"created_at.dayofyear\").count().sortby(\"tweet_id\", ascending=False)\n",
    "popular_days = most_popular_days[\"dayofyear\"]\n",
    "number_of_tweets_per_day = most_popular_days[\"tweet_id\"]\n",
    "mask = number_of_tweets_per_day > 3000\n",
    "print(f\"average number of tweets per day: {np.average(number_of_tweets_per_day.values)}\")\n",
    "for n, d in zip(number_of_tweets_per_day[mask].values, popular_days[mask].values):\n",
    "    print(f\"{n} tweets on day {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df29eb3-4c21-4a68-b4f6-d39430fbf745",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d58075-db84-4b75-84f6-ba49dfe34856",
   "metadata": {},
   "source": [
    "### Full unique dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d65fe3-66d2-4622-b8a4-e391582d548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_word = r\"(?:\\w[\\w']+)\"\n",
    "# 2+ consecutive punctuations, e.x. :)\n",
    "ascii_art = r\"(?:[{punctuation}][{punctuation}]+)\".format(punctuation=string.punctuation)\n",
    "# a single character that is not alpha_numeric or other ascii printable\n",
    "emoji = r\"(?:[^\\s])(?<![\\w{ascii_printable}])\".format(ascii_printable=string.printable)\n",
    "regexp = r\"{normal_word}|{ascii_art}|{emoji}\".format(normal_word=normal_word, ascii_art=ascii_art, emoji=emoji)\n",
    "dir_name = os.path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "text = \" \".join(ds_unique.text.values)\n",
    "# Generate a word cloud image\n",
    "# The Symbola font includes most emoji\n",
    "wc = wordcloud.WordCloud(font_path=font_path, regexp=regexp, width=800, height=400).generate(text)\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90910cd9-76b4-4ce5-aa23-5172ea210a21",
   "metadata": {},
   "source": [
    "### Dataset without big bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b067833-3f79-4ab3-a041-7324c65fdf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_word = r\"(?:\\w[\\w']+)\"\n",
    "# 2+ consecutive punctuations, e.x. :)\n",
    "ascii_art = r\"(?:[{punctuation}][{punctuation}]+)\".format(punctuation=string.punctuation)\n",
    "# a single character that is not alpha_numeric or other ascii printable\n",
    "emoji = r\"(?:[^\\s])(?<![\\w{ascii_printable}])\".format(ascii_printable=string.printable)\n",
    "regexp = r\"{normal_word}|{ascii_art}|{emoji}\".format(normal_word=normal_word, ascii_art=ascii_art, emoji=emoji)\n",
    "dir_name = os.path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "text = \" \".join(ds_red.text.values)\n",
    "# Generate a word cloud image\n",
    "# The Symbola font includes most emoji\n",
    "wc = wordcloud.WordCloud(font_path=font_path, regexp=regexp, width=800, height=400).generate(text)\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd7e49f-76a8-48e0-a2ee-6ba7c47cb3ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Distribution of keywords in tweets\n",
    "Unfortunately can only use emojis with cairo backend, which does not seem to plot inline..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5126daea-b609-4e06-9a93-d4d0ff315fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "%matplotlib inline\n",
    "\n",
    "text = \" \".join(ds_red.text.values)\n",
    "occurence = []\n",
    "for k in keywords:\n",
    "    occurence.append(text.count(k))\n",
    "\n",
    "import matplotlib, mplcairo\n",
    "\n",
    "matplotlib.use(\"module://mplcairo.tk\")\n",
    "prop = matplotlib.font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  # These two lines need to be set manually\n",
    "plt.rcParams[\"font.family\"] = prop.get_family()\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 10), constrained_layout=True)\n",
    "for i, log in enumerate([True, False]):\n",
    "    ax = axs[i]\n",
    "    plot = ax.bar(np.arange(len(occurence)), occurence)\n",
    "    labels = [\"{}\".format(x) for x in keywords]\n",
    "    for rect1, label in zip(plot, labels):\n",
    "        height = rect1.get_height()\n",
    "        ax.annotate(\n",
    "            label,\n",
    "            (rect1.get_x() + rect1.get_width() / 2, height + 5),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=16,\n",
    "            fontproperties=prop,\n",
    "            rotation=90,\n",
    "        )\n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"keywords\")\n",
    "    ax.set_ylabel(\"counts\")\n",
    "    if log:\n",
    "        ax.set_yscale(\"log\")\n",
    "plt.draw()\n",
    "plt.show()\n",
    "fig.savefig(figure_path / \"word_count_all.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd4c97-cf9f-41ed-8cd6-047ff42bc296",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# switching back to default backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254fbd5-180c-4127-abd4-b673e1e32712",
   "metadata": {},
   "source": [
    "## Tokenize words, also removing stopwords, urls,... with spacy package -> how to deal with emojis unclear (could replace, use pretrained model or include emojis as tokens + train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7abe0cd-821c-4a85-83e3-152e1f6d727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry run python -m spacy download en_core_web_sm\n",
    "import string\n",
    "import re\n",
    "import spacy\n",
    "import contextualSpellCheck\n",
    "import spacymoji\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    text = re.sub(r\"\\S*https?:\\S*\", \"\", text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    # remove strings starting with # or @\n",
    "    return re.sub(r\"@[A-Za-z0-9]+|#[A-Za-z0-9]+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "\n",
    "def remove_instagram_atsign(text):\n",
    "    return re.sub(r\"@\\S*[A-Za-z0-9\\s,]+$\", \"\", remove_links(text), flags=re.MULTILINE)\n",
    "\n",
    "\n",
    "def strip_links(text):\n",
    "    link_regex = re.compile(\n",
    "        \"((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)\",\n",
    "        re.DOTALL,\n",
    "    )\n",
    "    links = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], \", \")\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_links(text):\n",
    "    link_regex = re.compile(\n",
    "        \"((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)\",\n",
    "        re.DOTALL,\n",
    "    )\n",
    "    links = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def replace_emojis_with_words(text, emojis_dic):\n",
    "    # Function for converting emoticons into word\n",
    "    for emot in emojis_dic.keys():\n",
    "        text = re.sub(\n",
    "            \"{}\".format(emot),\n",
    "            \"_\".join(emojis_dic[emot].replace(\":\", \"\").split()),\n",
    "            text,\n",
    "            flags=re.MULTILINE,\n",
    "        )\n",
    "    return text\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, add_emojis=False, add_spellchecking=False):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.add_emojis = add_emojis\n",
    "        self.add_spellchecking = add_spellchecking\n",
    "\n",
    "        if self.add_emojis:\n",
    "            emoji_config = {\"merge_spans\": False}\n",
    "            self.nlp.add_pipe(\"emoji\", first=True, config=emoji_config)\n",
    "\n",
    "        df_emoji = pd.read_csv(\"../src/a2/data/emoji/emoji_df.csv\")\n",
    "        self.emojis = \"\".join(df_emoji.emoji.values)\n",
    "        self.emojis_dic = {\n",
    "            k: v for k, v in zip(df_emoji.emoji.values, df_emoji.name.values) if k not in [\"*️⃣\", \"*⃣\"]\n",
    "        }  # latter cause error for re.sub\n",
    "\n",
    "        if add_spellchecking:\n",
    "            self.nlp.add_pipe(\"contextual spellchecker\", config={\"max_edit_dist\": 5})\n",
    "\n",
    "        self.punctuations = string.punctuation\n",
    "\n",
    "        self.stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "    def tokenize(self, sentence, source, return_string=False, replace_emojis=False):\n",
    "        sentence = remove_hashtags(sentence)\n",
    "        sentence = remove_urls(sentence)\n",
    "        sentence = re.sub(r\"([{}])([{}])\".format(self.emojis, self.emojis), r\"\\1 \\2\", sentence)\n",
    "        sentence = replace_emojis_with_words(sentence, self.emojis_dic)\n",
    "        if source == \"Instagram\":\n",
    "            sentence = remove_instagram_atsign(sentence)\n",
    "\n",
    "        docs = self.nlp(sentence)\n",
    "        if self.add_emojis:\n",
    "            emojis = [token.text for token in docs if token._.is_emoji]\n",
    "        else:\n",
    "            emojis = []\n",
    "        if self.add_spellchecking:\n",
    "            tokens = docs._.outcome_spellCheck\n",
    "            logging.debug(f\"tokens outcome spellcheck: {tokens}\")\n",
    "            tokens = self.nlp(tokens)\n",
    "        else:\n",
    "            tokens = docs\n",
    "        # Lemmatize each token and convert each token into lowercase\n",
    "        tokens = [word.lemma_.lower().strip() if word.lemma_ != \"PROPN\" else word.lower_ for word in tokens]\n",
    "\n",
    "        # Remove stopwords\n",
    "        tokens = [word for word in tokens if word not in self.stopwords and word not in self.punctuations]\n",
    "\n",
    "        # remove empty tokens\n",
    "        tokens = [word for word in tokens if word != \"\"]\n",
    "\n",
    "        if not return_string:\n",
    "            return tokens + emojis\n",
    "        else:\n",
    "            sentence = \" \".join(tokens)\n",
    "            if self.add_emojis:\n",
    "                sentence += \" \" + \" \".join(emojis)\n",
    "            return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1171eaa9-5211-4ee5-bab3-e756829b5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # removes warning from hugging-face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054d5267-1a7c-4cbf-bb31-a6f77d8e8c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# parallelized but still quite inefficient... using 10_000 tweets here as already takes ~3 min\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "if __name__ == \"__main__\":\n",
    "    _tokenize = partial(tokenizer.tokenize, return_string=True)\n",
    "    with mp.Pool(processes=16) as pool:\n",
    "        tokens = pool.starmap(\n",
    "            _tokenize,\n",
    "            zip(ds_red.text.values[:10000], ds_red.source.values[:10000]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005c59b1-04e9-426d-8ea8-8d770f6e1165",
   "metadata": {},
   "source": [
    "## Visualize distribution of tokenized texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227439a9-842f-4c1e-b3f8-dd157a610e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_red = ds_red.isel(index=slice(0, 10000))  # only 10_000 tweets see above\n",
    "ds_red[\"tokens\"] = ([\"index\"], tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca113851-dcce-4e5f-8217-6a300165a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenized = \" \".join(ds_red.tokens.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b43db6-a94f-47b4-9831-354ec1ec1845",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = wordcloud.WordCloud(font_path=font_path, regexp=regexp, width=800, height=400).generate(text_tokenized)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b19d14-8578-41c5-b2da-f558e58ef7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba1fcb3-65f1-4fdd-8580-d46fa50f5fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "%matplotlib inline\n",
    "\n",
    "text = text_tokenized\n",
    "keywords_extended = [\"☀\"] + keywords\n",
    "occurence = []\n",
    "for k in keywords_extended:\n",
    "    occurence.append(text.count(k))\n",
    "\n",
    "import matplotlib, mplcairo\n",
    "\n",
    "matplotlib.use(\"module://mplcairo.tk\")\n",
    "prop = matplotlib.font_manager.FontProperties(fname=os.path.join(dir_name, \"fonts\", \"Symbola.ttf\"))\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  # These two lines need to be set manually\n",
    "plt.rcParams[\"font.family\"] = prop.get_family()\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 10), constrained_layout=True)\n",
    "for i, log in enumerate([True, False]):\n",
    "    ax = axs[i]\n",
    "    plot = ax.bar(np.arange(len(occurence)), occurence)\n",
    "    labels = [\"{}\".format(x) for x in keywords_extended]\n",
    "    for rect1, label in zip(plot, labels):\n",
    "        height = rect1.get_height()\n",
    "        ax.annotate(\n",
    "            label,\n",
    "            (rect1.get_x() + rect1.get_width() / 2, height + 5),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=16,\n",
    "            fontproperties=prop,\n",
    "            rotation=90,\n",
    "        )\n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"keywords\")\n",
    "    ax.set_ylabel(\"counts\")\n",
    "    if log:\n",
    "        ax.set_yscale(\"log\")\n",
    "fig.savefig(figure_path / \"word_count_tokenized.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00831314-fda7-4f12-8c56-4cfc3be28339",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# switching back to default backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4567f32-97d0-4fd6-b03c-dc2d8b29ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(ds, filename, delete_file=True):\n",
    "    new_header = [\n",
    "        header,\n",
    "        \"# dates: 2020-01-1T00:00:00.000Z-->2020-02-1T00:00:00.000Z\",\n",
    "    ]\n",
    "    with open(filename, \"x\") as f:\n",
    "        f.write(f\"{new_header[0]}\\n\")\n",
    "        f.write(f\"{new_header[1]}\\n\")\n",
    "        ds.to_csv(f)\n",
    "\n",
    "\n",
    "save_file(ds_red.to_pandas(), os.path.join(filepath, \"ds_all_tokenized.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a2-laF_Cm_L-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov  2 2022, 18:53:38) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a06658bfc983828e336131948d402e02953311214b959ca171c834c9d9b531a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
