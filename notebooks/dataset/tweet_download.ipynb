{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "960c6c94-603b-4308-9ee8-9f619d62f22a",
   "metadata": {},
   "source": [
    "# Download tweets via the request package from the twitter api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927ad351-3a9d-4a94-9274-f981831c36af",
   "metadata": {},
   "source": [
    "- Download tweets for training of application 2 for the maelstrom project. \n",
    "- Search based on:\n",
    "    - keywords based on vocabularies related to weather (from seperate files found in ../data/vocabularies/) including emojis provided by emoji pacakge\n",
    "    - date / time\n",
    "    - usually require geo spatial information\n",
    "- Includes quick analysis of \n",
    "    - time distribution of tweets\n",
    "    - source of tweets\n",
    "    - most active users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d775184-d7a3-42e0-8de7-8fc8f0eb66d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows update of external libraries without need to reload package\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e048f964-c6fa-466d-8ea6-47999fde1d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import logging\n",
    "from collections import Counter\n",
    "import os\n",
    "import glob\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import emoji\n",
    "import a2.twitter.downloader\n",
    "import a2.dataset.load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a5f79f-4a7b-4117-84b0-1440f2cafa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tweets(filename=\"test.csv\", n_sample=10, n=None):\n",
    "    \"\"\"load tweets in filename and print random n_sample or n tweets from beginning of file\"\"\"\n",
    "    df = pd.read_csv(filename, skiprows=2).to_xarray()\n",
    "    df[\"created_at\"] = ([\"index\"], pd.to_datetime(df.created_at).values)\n",
    "    if n is None:\n",
    "        index_list = [random.randrange(df[\"tweet_id\"].shape[0]) for i in range(n_sample)]\n",
    "    elif n == \"all\":\n",
    "        index_list = [i for i in range(df[\"tweet_id\"].shape[0])]\n",
    "    else:\n",
    "        index_list = [i for i in range(n)]\n",
    "    for i in index_list:\n",
    "        print(\"------------------------------\")\n",
    "        print(df.text.values[i])\n",
    "        print(\"------------------------------\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def print_author_text_source(df):\n",
    "    for author, tweet_text, source in zip(df[\"author_id\"].values, df[\"text\"].values, df[\"source\"].values):\n",
    "        tweet_text = tweet_text.replace(\"\\n\", \"\")\n",
    "        print(f\"from: {author}, {source}\\n    text: {tweet_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef7062-06cd-4d4d-9b07-c37abf887805",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emoji = pd.read_csv(\"../../src/a2/data/emoji/emoji_df.csv\")\n",
    "df_emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e91f68-31ee-48a7-9e74-7c226c6fa17e",
   "metadata": {},
   "source": [
    "## Prepare vocab for search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9900bcb5-ebcd-4ba7-9481-6d3d1d6c2169",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = a2.twitter.downloader.get_vocabulary()\n",
    "vocab = [re.sub(r\" \\(.*\\)\", \"\", x) for x in vocab][0:64]\n",
    "vocab_string = \" OR \".join(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742299e-fafe-4c52-bb9b-cae5bb29e8e8",
   "metadata": {},
   "source": [
    "##  Using emoji package to include emojis in search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9655cf-bfbf-47bd-95d2-c2eae4182d35",
   "metadata": {},
   "source": [
    "pick emojis with name that matches words in vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6397f0-66e0-4f41-b1fd-33289c8bcef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_list = a2.twitter.downloader.get_emojis_from_vocab(vocab, exclude=[\"rainbow flag\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9efc53-3029-436d-953b-87e9ba806acf",
   "metadata": {},
   "source": [
    "## Run search query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c64a40c6-3d04-49f6-b7cc-bdd45007e823",
   "metadata": {},
   "source": [
    "Would like to connect information about the weather provided in the tweet with the location of the person tweeting. Therefore geospatial location is crucial.\n",
    "The following search terms appear sensible for this project:\n",
    "- `has:geo` \n",
    "    - enforces that the user either (1) activated tracking on his device such that GPS coordinates are directly available or (2) tagged his tweet from a list of possible locations provided by [foursquare](https://twittercommunity.com/t/foursquare-location-data-in-the-api/36065) with varying *levels* (state, county, city, ...)\n",
    "    - additional information provided by [twitter](https://developer.twitter.com/en/docs/tutorials/advanced-filtering-for-geo-data)\n",
    "- `-is:retweet`\n",
    "    - is *not* a retweet as they cannot have locations attached, already filtered out by `has:geo`\n",
    "- `lang:en`\n",
    "    - tweets are tagged with language or marked undefined `und`\n",
    "- `place_country:GB`\n",
    "    - country of origin (look at GB as rather active on twitter)\n",
    "- `-is:nullcast`\n",
    "    - removes tweets that are for promotion only \n",
    "- `-from:3446146816`\n",
    "    - exclude specific user, userid 3446146816 produces massive amounts of tweets about flood warnings, which are expected to bias the model \n",
    "\n",
    "See overview [listing of operators](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query) provided by twitter.\n",
    "\n",
    "In addition, we specify\n",
    "- `start_dates`\n",
    "- `end_dates`\n",
    "- `max_results`\n",
    "    - twitter only provides a maximum of 500 tweets, our library tweets_downloader takes care of this, however we limit total number of downloads per query to not accidently exceed our quota of 10M per month due to excessive spamming of a user. \n",
    "    - that's also why currently limit query to single month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104ab4ad-a474-4bb5-9951-3362aeb74c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = f\"sun has:geo -is:retweet (lang:en) place_country:GB -is:nullcast -from:3446146816\"\n",
    "keyword = \"sun -is:retweet (lang:en) -is:nullcast -from:3446146816\"\n",
    "\n",
    "start_dates = \"2020-01-1T10:00:10.000Z\"\n",
    "end_dates = \"2020-01-1T12:00:00.000Z\"\n",
    "end_dates = \"2020-01-1T10:05:00.000Z\"\n",
    "filename = \"fake_not_enought_tweets\"\n",
    "max_results = 600\n",
    "print(keyword)\n",
    "a2.twitter.downloader.download_tweets(\n",
    "    filepath=filename,\n",
    "    keyword=keyword,\n",
    "    start_dates=start_dates,\n",
    "    end_dates=end_dates,\n",
    "    max_results=max_results,\n",
    "    sleep_time=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520414d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date(year, month=1, day=1):\n",
    "    return \"%i-%02d-%02dT00:00:00.000Z\" % (year, month, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6ef92-b99e-4b60-ae18-90e0524af36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove known bots via their userid `weather_bots`.\n",
    "weather_bots = (\n",
    "    \"-from:3446146816\"\n",
    "    \" -from:2522135204 -from:4643841555 -from:19711240 -from:2581881138 -from:26643647 -from:23366418 -from:2411260615 -from:1035516433 -from:88720351 -from:470441737\"\n",
    "    \" -from:3029396645 -from:20788211 -from:186672208 -from:161831709\"\n",
    ")\n",
    "ads = \"-from:824637752574488576\"\n",
    "bot_terms = [\n",
    "    \"Hum\",\n",
    "    \"Press\",\n",
    "    \"Barometer\",\n",
    "    \"Pressure\",\n",
    "    \"Humidity\",\n",
    "    \"Baro\",\n",
    "    \"Humid\",\n",
    "]\n",
    "keyword = (\n",
    "    f'({\" OR \".join(emoji_list)} OR {\" OR \".join(vocab)}) has:geo -is:retweet (lang:en) place_country:GB -is:nullcast '\n",
    "    + weather_bots\n",
    "    + \" \"\n",
    "    + ads\n",
    "    + \" \"\n",
    "    + \"\".join([\"-\" + t + \" \" for t in bot_terms])\n",
    ")\n",
    "max_results = 200_000\n",
    "year = 2014\n",
    "for month in range(1, 13):\n",
    "    start_dates = format_date(year, month, 1)\n",
    "    end_dates = format_date(year, month + 1, 1)\n",
    "    if month == 12:\n",
    "        end_dates = format_date(year + 1, 1, 1)\n",
    "    filename = \"tweets_%i_%02d\" % (year, month)\n",
    "    print(keyword)\n",
    "    a2.twitter.downloader.download_tweets(\n",
    "        filepath=filename,\n",
    "        keyword=keyword,\n",
    "        start_dates=start_dates,\n",
    "        end_dates=end_dates,\n",
    "        max_results=max_results,\n",
    "        sleep_time=0.1,\n",
    "    )\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cd63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b48fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_bots = (\n",
    "    \"-from:3446146816\"\n",
    "    \" -from:2522135204 -from:4643841555 -from:19711240 -from:2581881138 -from:26643647 -from:23366418 -from:2411260615 -from:1035516433 -from:88720351 -from:470441737\"\n",
    "    \" -from:3029396645 -from:20788211 -from:186672208 -from:161831709\"\n",
    ")\n",
    "ads = \"-from:824637752574488576\"\n",
    "keyword = (\n",
    "    f\"has:geo -is:retweet (lang:en) place_country:GB -is:nullcast \"\n",
    "    # + weather_bots\n",
    "    # + \" \"\n",
    "    # + ads\n",
    ")\n",
    "max_results = 200_000\n",
    "year = 2020\n",
    "month = 2\n",
    "for day in range(14, 24):\n",
    "    start_dates = \"%i-%02d-%02dT00:00:00.000Z\" % (year, month, day)\n",
    "    end_dates = \"%i-%02d-%02dT00:00:00.000Z\" % (year, month, day + 1)\n",
    "    if month == 12:\n",
    "        end_dates = \"%i-%02d-01T00:00:00.000Z\" % (year + 1, 1)\n",
    "    filename = f\"tweets_no_keywords_{start_dates}_{end_dates}\"\n",
    "    print(keyword)\n",
    "    a2.twitter.downloader.download_tweets(\n",
    "        filepath=filename,\n",
    "        keyword=keyword,\n",
    "        start_dates=start_dates,\n",
    "        end_dates=end_dates,\n",
    "        max_results=max_results,\n",
    "        sleep_time=0.1,\n",
    "    )\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a1f1c1",
   "metadata": {},
   "source": [
    "## Check downloaded Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f65ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = filename + \".json\"\n",
    "ds = a2.dataset.load_dataset.load_tweets_dataframe_from_jsons([json_filename]).to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d400e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tweet_authors(ds):\n",
    "    authors = ds.groupby(\"author_id\").count().sortby(\"id\", ascending=False)\n",
    "    for i, (n, a) in enumerate(zip(authors[\"id\"].values, authors[\"author_id\"].values)):\n",
    "        if i > 20:\n",
    "            break\n",
    "        print(f\"{a} --> {n}\")\n",
    "        mask = ds.author_id.values == a\n",
    "        print(f\"sample: {ds['text'].loc[mask].values}\")\n",
    "\n",
    "\n",
    "print_tweet_authors(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0af6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e094bdb4-ae7a-4f6b-ad05-322406f69a10",
   "metadata": {},
   "source": [
    "## Check for occurence of bots and most active users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ead567a-93f4-4b12-b683-1ca14eef79e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../data/tweets/gb_2017_rain_sun_vocab_emojis/\"\n",
    "path = os.path.abspath(filepath)  # use your path\n",
    "all_files = glob.glob(os.path.join(path, \"tweets_2017*.json\"))\n",
    "\n",
    "# ds = a2.dataset.load_dataset.load_tweets_dataframe_from_jsons(\n",
    "#     all_files\n",
    "# ).to_xarray()\n",
    "ds = a2.dataset.load_dataset.load_tweets_dataframe_from_json(\"tweets_2018_01.json\").to_xarray()\n",
    "n_old = ds.index.shape[0]\n",
    "ds = ds.where(\n",
    "    (ds.source == \"Twitter for iPhone\")\n",
    "    | (ds.source == \"Twitter for Android\")\n",
    "    | (ds.source == \"Instagram\")\n",
    "    | (ds.source == \"Twitter for iPad\")\n",
    "    | (ds.source == \"Twitter Web Client\"),\n",
    "    drop=True,\n",
    ")\n",
    "sources_non_bot = [\n",
    "    \"Twitter for iPhone\",\n",
    "    \"Twitter for Android\",\n",
    "    \"Instagram\",\n",
    "    \"Twitter for iPad\",\n",
    "    \"Twitter Web Client\",\n",
    "]\n",
    "n_new = ds.index.shape[0]\n",
    "print(f\"initial size dataset: {n_old}, removed {n_old-n_new} 'bots', new size: {n_new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f28f1ce-97f6-423c-aeec-d22a734fbd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = ds.groupby(\"author_id\").count().sortby(\"id\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94887dd1-870d-4cbc-bf60-76e4b7c8872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tweet_authors(ds):\n",
    "    authors = ds.groupby(\"author_id\").count().sortby(\"id\", ascending=False)\n",
    "    for i, (n, a) in enumerate(zip(authors[\"id\"].values, authors[\"author_id\"].values)):\n",
    "        if i > 20:\n",
    "            break\n",
    "        print(f\"{a} --> {n}\")\n",
    "        mask = ds.author_id.values == a\n",
    "        unique_sources = np.unique(ds[\"source\"].loc[mask].values)\n",
    "        print(f\"source: {unique_sources[0] if len(unique_sources) else unique_sources}\")\n",
    "        print(f\"sample: {np.random.choice(ds['text'].loc[mask].values, 5 if n>5 else n, replace=False)}\")\n",
    "\n",
    "\n",
    "print_tweet_authors(ds.where(ds.text.str.contains(f\"{'|'.join(bot_terms)}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf74ae4-dcf8-4d35-b4af-c07e22bb1aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = ds.groupby(\"source\").count().sortby(\"id\", ascending=False)\n",
    "for n, s in zip(sources[\"id\"].values, sources[\"source\"].values):\n",
    "    print(f\"{s} --> {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6463ff69-aee5-49b7-9c78-792a48530969",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 15))\n",
    "ax = plt.axes()\n",
    "ds.groupby(\"source\").count().plot.scatter(y=\"source\", x=\"id\", figsize=(20, 20), ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e324e559-4176-4579-bb94-65e2ee6f0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tweet_authors(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54e3f4b-4fcb-4db6-aee1-9d9a5ae58886",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tweet_authors(ds.where(~ds.source.str.contains(f\"{'|'.join(sources_non_bot)}\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d03d5fa-ac99-46c9-bbc5-db6f4f327eec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Quick analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b0c31e-30fc-49e6-923f-cad0da1f899f",
   "metadata": {},
   "source": [
    "## Number of tweets per user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04494c23-089d-4fec-8e46-67190db9c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = print_tweets(filename=\"test.csv\", n_sample=10, n=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007a30c9-0b6e-44dd-810f-3faf29eb9dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"author_id\").count()[\"tweet_id\"].plot.hist(bins=100)\n",
    "ax = plt.gca()\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"occurence of author_id\")\n",
    "ax.set_ylabel(\"count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab1c10-c864-4895-8aa6-322208ea430a",
   "metadata": {},
   "source": [
    "## Source of tweets: Private weather stations usually have source: pywws / MeteoWare Plus+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f6b08e-77b7-46c4-b906-5bb0edc21efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"source\").count().plot.scatter(y=\"source\", x=\"tweet_id\", size=10)\n",
    "ax = plt.gca()\n",
    "ax.tick_params(axis=\"x\", labelrotation=0)\n",
    "ax.set_xlabel(\"source\")\n",
    "ax.set_ylabel(\"count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c71fe3-c8b4-4b93-aa01-55f0bbc0118e",
   "metadata": {},
   "source": [
    "## Number of tweets per day of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4506ad81-7a8d-4aeb-83bf-7acd0f1773c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"created_at.dayofyear\").count().plot.scatter(x=\"dayofyear\", y=\"author_id\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55d2e4-551c-4edf-b382-68795915401b",
   "metadata": {},
   "source": [
    "## Look at most active users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649cc084-4755-4519-9dde-b472933caac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity = df.groupby(\"author_id\").count().sortby(\"tweet_id\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee717b4-b4d4-44d7-a461-cd244152e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_activity_sorted_by_activity = activity[\"author_id\"]\n",
    "number_of_tweets = activity[\"tweet_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01bcc71-dbef-46c9-8432-ad67dce48783",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_activity_sorted_by_activity[number_of_tweets > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404abdef-f881-4e30-8dd0-c20645e8ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = number_of_tweets > 2\n",
    "for user_id, n_tweets in zip(user_activity_sorted_by_activity[mask].values, number_of_tweets[mask].values):\n",
    "    user = a2.data_manipulation.twitter.downloader.get_user_from_userid(user_id)[\"data\"]\n",
    "    print(\n",
    "        f'{user[\"name\"]}, @{user[\"username\"]}, {user[\"location\"] if \"location\" in user else \"?\"} --> {n_tweets} tweets'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff02004-bd13-4a1b-a893-3be7e54270f7",
   "metadata": {},
   "source": [
    "## Example of \n",
    "### - obtaining user information from user id\n",
    "### - getting location from place id\n",
    "### - converting tweets in json format to list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f71d8-d99c-4e07-8f73-da8ece693235",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = a2.data_manipulation.twitter.downloader.get_user_from_userid(375106238)\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7850d50-0977-4e44-be17-5b2b2db7d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2.data_manipulation.twitter.manipulate_tweets.convert_single_tweet_to_list(j, \"user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9dbbb5-ef76-4ab9-80b0-cdeaa98b3d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = a2.data_manipulation.twitter.downloader.get_location_from_placeid(\"78e87ea8817310a6\")\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c165330-96f7-41fb-9bda-fcb4131ac88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2.data_manipulation.twitter.manipulate_tweets.convert_single_tweet_to_list(tweet, \"location\", skip_fields=[\"country\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f70cad3-7280-4cd0-9145-1de453c21559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "06c3f19a037da30281d10f8fbda7f2df02465bd919ed005b52297a6fc2623835"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
